{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Mapping\n",
    "+ GOAL: Map problem descriptions in TAAS-INFO to trouble tickets in MRDB to expedite diagnoses\n",
    "+ METHOD: \n",
    "> 1. Preprocessing: Remove stop words, lemmatization, \n",
    "> 2. Filter to most common unigrams\n",
    "> 3. Vectorization by Bag-of-words or TF-IDF\n",
    "> 4. Generate topics by Singular Value Decomp or Latent Dirichlet Allocation\n",
    "> 5. Cluster problem descriptions and solutions\n",
    "> 6. Trouble ticket mapped to k nearest neighbors by Euclidean or Mahalanobis distance\n",
    "> 7. n problems are sampled from the problem clusters of dataset 2\n",
    "> 8. Those solutions are mapped to clusters and aggregated to make recommendation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in stopwords.words('english'), spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdb_filename = \"Filename1.xlsx\"\n",
    "taas_filename = \"Filename2.xls\"\n",
    "mrdb_field = \"Problem Description Fieldname\"\n",
    "taas_field = \"Problem Description Fieldname\"\n",
    "mrdb_solution_field = \"Solution Fieldname\"\n",
    "taas_solution_field = \"Solution Fieldname\"\n",
    "\n",
    "mrdb_df = pd.read_excel(mrdb_filename)\n",
    "taas_df = pd.read_excel(taas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_consider = 5000\n",
    "topics_to_consider = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 100\n",
    "long_mrdb_df = mrdb_df[mrdb_df[mrdb_field].str.len() > threshold]\n",
    "long_mrdb_df.reset_index(inplace=True)\n",
    "long_taas_df = taas_df[taas_df[taas_field].str.len() > threshold]\n",
    "long_taas_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taas_tokenized = []\n",
    "for d in long_taas_df[taas_field]:\n",
    "    taas_tokenized.append(nlp(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdb_tokenized = []\n",
    "for d in long_mrdb_df[mrdb_field]:\n",
    "    mrdb_tokenized.append(nlp(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_unigrams = defaultdict(int)\n",
    "lemmatized = []\n",
    "for doc in mrdb_tokenized:\n",
    "    text = []\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct or t.is_space: \n",
    "            continue\n",
    "        if t.lemma_ == \"'s\":\n",
    "            continue\n",
    "        common_unigrams[t.lemma_] += 1\n",
    "        text.append(t.lemma_)\n",
    "    lemmatized.append(' '.join(text))\n",
    "    \n",
    "#long_mrdb_df[mrdb_field] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "for doc in taas_tokenized:\n",
    "    text = []\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct or t.is_space: \n",
    "            continue\n",
    "        if t.lemma_ == \"'s\":\n",
    "            continue\n",
    "        \n",
    "        text.append(t.lemma_)\n",
    "    lemmatized.append(' '.join(text))\n",
    "    \n",
    "long_taas_df[taas_field] = lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def check(neighbor, idx, data, src_df, src_text, dest_df, dest_text):\n",
    "\n",
    "    distances, indices = neighbor.kneighbors(np.expand_dims(data[idx], axis=0))\n",
    "    print(src_df.iloc[idx][src_text])\n",
    "    prev_dist = 0\n",
    "    for i in range(len(indices[0])):\n",
    "        #if math.isclose(prev_dist, distances[0][i]):\n",
    "        #    continue\n",
    "        prev_dist = distances[0][i]\n",
    "        print()\n",
    "        print(\"Result {}----------------\".format(i+1))\n",
    "        print(\"MRDB Trouble Ticket Number: \", dest_df.iloc[indices[0][i]][\"MRDB Trouble Ticket Number\"], \" Distance: \", distances[0][i], \"Index: \", indices[0][i])\n",
    "        print(dest_df.iloc[indices[0][i]][dest_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = sorted([(v, k) for k, v in common_unigrams.items()], reverse=True)\n",
    "feature_vec = [v for count, v in rv[:words_to_consider]]\n",
    "vectorizer = CountVectorizer(vocabulary=feature_vec)\n",
    "mrdb_data = vectorizer.fit_transform(long_mrdb_df[mrdb_field])\n",
    "taas_data = vectorizer.fit_transform(long_taas_df[taas_field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = sorted([(v, k) for k, v in common_unigrams.items()], reverse=True)\n",
    "feature_vec = [v for count, v in rv[:words_to_consider]]\n",
    "vectorizer = TfidfVectorizer(vocabulary=feature_vec)\n",
    "merged = long_mrdb_df[mrdb_field].append(long_taas_df[taas_field])\n",
    "merged_data = vectorizer.fit_transform(merged)\n",
    "mrdb_data = merged_data[:len(long_mrdb_df[mrdb_field])]\n",
    "taas_data = merged_data[len(long_mrdb_df[mrdb_field]):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Demonstration with 3 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda3 = LatentDirichletAllocation(n_components=3, random_state=0, topic_word_prior=1, doc_topic_prior=1)\n",
    "lda3.fit(mrdb_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 100 words in 3 topics\n",
    "\n",
    "maximums = np.argmax(lda3.exp_dirichlet_component_[:,:], axis=0)\n",
    "topics = defaultdict(list)\n",
    "\n",
    "for i in range(100):\n",
    "    topics[maximums[i]].append(feature_vec[i]) \n",
    "    \n",
    "for i in range(3):\n",
    "    print(\"Topic {}\". format(i+1), topics[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdb_transformed = lda3.transform(mrdb_data)\n",
    "taas_transformed = lda3.transform(taas_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all three topics\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import rc\n",
    "\n",
    "blue = (4/255, 47/255, 85/255, 255/255)\n",
    "red = (218/255, 4/255, 36/255, 255/255)\n",
    "length, _ = mrdb_transformed.shape\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "c = blue\n",
    "m = '.'\n",
    "xs = taas_transformed[:,0]\n",
    "ys = taas_transformed[:,1]\n",
    "zs = taas_transformed[:,2]\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m, label=\"TAAS-INFO\")\n",
    "\n",
    "c = red\n",
    "m = '.'\n",
    "xs = mrdb_transformed[:,0]\n",
    "ys = mrdb_transformed[:,1]\n",
    "zs = mrdb_transformed[:,2]\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m, label=\"MRDB\")\n",
    "\n",
    "blue_patch = mpatches.Patch(color=blue, label='TAAS-INFO')\n",
    "red_patch = mpatches.Patch(color=red, label='MRDB')\n",
    "plt.legend(handles=[red_patch, blue_patch])\n",
    "font = {'size'   : 18}\n",
    "rc('font', **font)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "for t in ax.zaxis.get_major_ticks(): t.label.set_fontsize(14)\n",
    "\n",
    "ax.set_xlabel('Topic 1', fontsize=18, x=0.5,y=-1)\n",
    "ax.set_ylabel('Topic 2', fontsize=18)\n",
    "ax.set_zlabel('Topic 3', fontsize=18)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot pairwise comparison of topics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "mc = red\n",
    "m = '.'\n",
    "mxs = mrdb_transformed[:,0]\n",
    "mys = mrdb_transformed[:,1]\n",
    "mzs = mrdb_transformed[:,2]\n",
    "\n",
    "tc = blue\n",
    "m = '.'\n",
    "txs = taas_transformed[:,0]\n",
    "tys = taas_transformed[:,1]\n",
    "tzs = taas_transformed[:,2]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,20))\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(3.2)\n",
    "fig.set_figwidth(13)\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "ax.scatter(mxs, mys, c=mc, marker=m)\n",
    "ax.scatter(txs, tys, c=tc, marker=m)\n",
    "ax.set_xlabel(\"Topic 1\", fontsize=18)\n",
    "ax.set_ylabel(\"Topic 2\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(mys, mzs, c=mc, marker=m)\n",
    "ax2.scatter(tys, tzs, c=tc, marker=m)\n",
    "ax2.set_xlabel(\"Topic 2\", fontsize=18)\n",
    "ax2.set_ylabel(\"Topic 3\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(mxs, mzs, c=mc, marker=m)\n",
    "ax3.scatter(txs, tzs, c=tc, marker=m)\n",
    "ax3.set_xlabel(\"Topic 1\", fontsize=18)\n",
    "ax3.set_ylabel(\"Topic 3\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "blue_patch = mpatches.Patch(color=blue, label='TAAS-INFO')\n",
    "red_patch = mpatches.Patch(color=red, label='MRDB')\n",
    "plt.legend(handles=[red_patch, blue_patch])\n",
    "font = {'size'   : 18}\n",
    "rc('font', **font)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=topics_to_consider, doc_topic_prior=1, max_iter=100, random_state=0)\n",
    "lda.fit(mrdb_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mrdb_transformed[0].min())\n",
    "print(mrdb_transformed[0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdb_transformed = lda.transform(mrdb_data)\n",
    "taas_transformed = lda.transform(taas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor by Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_nbrs = NearestNeighbors(n_neighbors=10, \n",
    "                            algorithm='ball_tree', \n",
    "                            metric='euclidean').fit(mrdb_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_mrdb_df[mrdb_solution_field].iloc[3019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(lda_nbrs, 8554, taas_transformed, long_taas_df, taas_field, long_mrdb_df, mrdb_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor by Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = np.linalg.pinv(np.cov(mrdb_transformed.T))\n",
    "lda_maha_nbrs = NearestNeighbors(n_neighbors=10, \n",
    "                            algorithm='brute', \n",
    "                            metric='mahalanobis', \n",
    "                            metric_params={'VI': vi}).fit(mrdb_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(lda_maha_nbrs, 8554, taas_transformed, long_taas_df, taas_field, long_mrdb_df, mrdb_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([0, 0])\n",
    "cov = np.array([[9, 0], [0, 1]])\n",
    "np.random.multivariate_normal(m, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i in range(10000):\n",
    "    x1, y1 =np.random.multivariate_normal(m, cov)\n",
    "    x.append(x1)\n",
    "    y.append(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "pt = [-10, -8, -9]\n",
    "pty = [0, 0, 1]\n",
    "\n",
    "plt.figure(figsize=(21, 7))\n",
    "plt.scatter(x, y, c='black', marker='.')\n",
    "plt.scatter(pt, pty, c='red', marker='o')\n",
    "plt.xticks([i-15 for i in range(25)])\n",
    "plt.annotate('A', (pt[0], pty[0]), fontsize='xx-large', bbox=dict(facecolor='blue', alpha=0.2))\n",
    "plt.annotate('B', (pt[1], pty[1]), fontsize='xx-large', bbox=dict(facecolor='blue', alpha=0.2))\n",
    "plt.annotate('C', (pt[2], pty[2]), fontsize='xx-large', bbox=dict(facecolor='blue', alpha=0.2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition Demonstration with 3 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(mrdb_data.toarray(), full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smat = np.diag(s)\n",
    "topic_mat = np.dot(smat, vh)\n",
    "taas_topic_map = np.dot(taas_data.toarray(), np.linalg.inv(topic_mat)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic of first 100 words\n",
    "\n",
    "maximums = np.argmax(topic_mat[:3,:], axis=0)\n",
    "\n",
    "\n",
    "topics = defaultdict(list)\n",
    "\n",
    "for i in range(100):\n",
    "    topics[maximums[i]].append(feature_vec[i]) \n",
    "    \n",
    "for i in range(3):\n",
    "    print(\"Topic {}\". format(i+1), topics[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot full dataset\n",
    "\n",
    "length, _ = mrdb_transformed.shape\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\"\"\n",
    "c = red\n",
    "m = '.'\n",
    "xs = u[:,1]\n",
    "ys = u[:,2]\n",
    "zs = u[:,3]\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m, label=\"MRDB\")\n",
    "\n",
    "c = blue\n",
    "m = '.'\n",
    "xs = taas_topic_map[:,1]\n",
    "ys = taas_topic_map[:,2]\n",
    "zs = taas_topic_map[:,3]\n",
    "ax.scatter(xs, ys, zs, c=c, marker=m, label=\"TAAS-INFO\")\n",
    "\n",
    "ax.set_xlabel('Topic 1')\n",
    "ax.set_ylabel('Topic 2')\n",
    "ax.set_zlabel('Topic 3')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "mc = red\n",
    "m = '.'\n",
    "mxs = u[:,1]\n",
    "mys = u[:,2]\n",
    "mzs = u[:,3]\n",
    "\n",
    "tc = blue\n",
    "m = '.'\n",
    "txs = taas_topic_map[:,1]\n",
    "tys = taas_topic_map[:,2]\n",
    "tzs = taas_topic_map[:,3]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3,20))\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax = fig.add_subplot(131)\n",
    "ax.scatter(mxs, mys, c=mc, marker=m)\n",
    "ax.scatter(txs, tys, c=tc, marker=m)\n",
    "ax.set_xlabel(\"Topic 1\", fontsize=18)\n",
    "ax.set_ylabel(\"Topic 2\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.scatter(mys, mzs, c=mc, marker=m)\n",
    "ax2.scatter(tys, tzs, c=tc, marker=m)\n",
    "ax2.set_xlabel(\"Topic 2\", fontsize=18)\n",
    "ax2.set_ylabel(\"Topic 3\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.scatter(mxs, mzs, c=mc, marker=m)\n",
    "ax3.scatter(txs, tzs, c=tc, marker=m)\n",
    "ax3.set_xlabel(\"Topic 1\", fontsize=18)\n",
    "ax3.set_ylabel(\"Topic 3\", fontsize=18)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor by Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = topics_to_consider\n",
    "svd_nbrs = NearestNeighbors(n_neighbors=10, \n",
    "                            algorithm='ball_tree', \n",
    "                            metric='euclidean').fit(taas_topic_map[:,:t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_mrdb_df[mrdb_solution_field].iloc[76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(svd_nbrs, 8554, u[:,:t], long_taas_df, taas_field, long_mrdb_df, mrdb_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor by Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_maha_nbrs = NearestNeighbors(n_neighbors=10, \n",
    "                            algorithm='brute', \n",
    "                            metric='mahalanobis',\n",
    "                            metric_params={'V': np.cov(u[:,:t].T)}).fit(u[:,:t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(svd_maha_nbrs, 8554, u[:,:t], long_taas_df, taas_field, long_mrdb_df, mrdb_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics on Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spelling on some mistakes\n",
    "def fix_spelling(s):\n",
    "    s = str(s).strip()\n",
    "    return s.replace('RECIEVED', 'RECEIVED').replace('RECIEVE', 'RECEIVE').replace('RECIVED', 'RECEIVED')\n",
    "\n",
    "long_mrdb_df['solution'] = long_mrdb_df[mrdb_solution_field].map(lambda x: fix_spelling(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrdb_sol_tokenized = []\n",
    "for d in long_mrdb_df['solution']:\n",
    "    mrdb_sol_tokenized.append(nlp(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_sol_unigrams = defaultdict(int)\n",
    "lemmatize = []\n",
    "count_total = 0\n",
    "replaced_total = 0\n",
    "null_total = 0\n",
    "reset_total = 0\n",
    "extra = []\n",
    "\n",
    "for doc in mrdb_sol_tokenized:\n",
    "    count_total += 1\n",
    "    sol = []\n",
    "    if doc.__len__() <= 1:\n",
    "        null_total += 1\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct or t.is_space: \n",
    "            continue\n",
    "        if t.lemma_ == \"'s\":\n",
    "            continue\n",
    "        common_sol_unigrams[t.lemma_] += 1\n",
    "        sol.append(t.lemma_)\n",
    "    \n",
    "    lemmatize.append(' '.join(sol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solution topics\n",
    "\n",
    "rv = sorted([(v, k) for k, v in common_sol_unigrams.items()], reverse=True)\n",
    "feature_vec = [v for count, v in rv[:2000]]\n",
    "vectorizer = TfidfVectorizer(vocabulary=feature_vec)\n",
    "solution_data = vectorizer.fit_transform(lemmatize)\n",
    "\n",
    "# Map solution dataset to topics\n",
    "sol_lda = LatentDirichletAllocation(n_components=200, doc_topic_prior=1, max_iter=50, random_state=0)\n",
    "sol_lda.fit(solution_data) \n",
    "\n",
    "sol_lda_data = sol_lda.transform(solution_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = defaultdict(int)\n",
    "\n",
    "for doc in mrdb_sol_tokenized:\n",
    "    for t in doc:\n",
    "        if t.pos_ == 'VERB':\n",
    "            verbs[t.lemma_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_verbs = sorted([(v, k) for k, v in verbs.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted list of tuples (counts, verbs)\n",
    "# this information has been removed\n",
    "cleaned_sorted = [\n",
    " \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = [x[1] for x in cleaned_sorted]\n",
    "sizes = [x[0] for x in cleaned_sorted]\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "size = 0.3\n",
    "# vals is list of lists of counts\n",
    "vals = []\n",
    "# labels is numpy array of list of verbs\n",
    "labels = np.array()\n",
    "\n",
    "sum_vals = [sum(x) for x in vals]\n",
    "flatten_vals = []\n",
    "for x in vals:\n",
    "    flatten_vals += x\n",
    "    \n",
    "flatten_labels = []\n",
    "for x in labels:\n",
    "    flatten_labels += x\n",
    "    \n",
    "cmap = plt.get_cmap(\"Set3\")\n",
    "ygb_cmap = plt.get_cmap(\"YlGnBu\")\n",
    "outer_colors = cmap([1, 10, 0, 4])\n",
    "inner_colors = ygb_cmap(np.arange(28)*10)\n",
    "\n",
    "ax.pie(sum_vals, radius=1, colors=outer_colors,\n",
    "       wedgeprops=dict(width=size, edgecolor='w'))\n",
    "\n",
    "ax.pie(flatten_vals, radius=1-size, labels=flatten_labels, colors=inner_colors, autopct='%1.1f%%',\n",
    "       wedgeprops=dict(width=size, edgecolor='w'))\n",
    "\n",
    "ax.set(aspect=\"equal\", title='Solutions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial TAAS-INFO to Nearest Solution and Nearest Cluster Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Cluster solution topics\n",
    "sol_kmeans = KMeans(n_clusters=k, random_state=0).fit(sol_lda_data)\n",
    "sol_cluster_nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(sol_kmeans.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(neighbor, idx, data, src_df, src_text, dest_df, dest_text):\n",
    "    distances, indices = neighbor.kneighbors(np.expand_dims(data[idx], axis=0))\n",
    "    return indices[0]\n",
    "\n",
    "def get_recommendations(neighbor, indices, dest_df, dest_text, absolute=True):\n",
    "    \"\"\" \n",
    "        Absolute True returns the exact solutions applied for the neighbors\n",
    "        Absolute False returns the solutions of the clusters \n",
    "    \"\"\"\n",
    "    sols = []\n",
    "    if absolute:\n",
    "        for i in range(len(indices)):\n",
    "            sols.append(dest_df[dest_text].iloc[indices[i]])\n",
    "        return sols\n",
    "    else:\n",
    "        # Map to centroid\n",
    "        cluster_indices = []\n",
    "        for i in range(len(indices)):\n",
    "            topic_sol = sol_lda.transform(solution_data[indices[i]])\n",
    "            dist, cluster_index = neighbor.kneighbors(topic_sol)\n",
    "            sols.append(cluster_map[cluster_index[0][0]])\n",
    "        return sols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idc  = get_neighbors(lda_nbrs, 8554, taas_transformed, long_taas_df, taas_field, long_mrdb_df, mrdb_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map TAAS-INFO problem description to neighbors in MRDB and their solutions\n",
    "absolute_sols = get_recommendations(sol_cluster_nbrs, idc, long_mrdb_df, mrdb_solution_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map TAAS-INFO problem description to neighbors in MRDB and their solution clusters\n",
    "clusters_sols = get_recommendations(sol_cluster_nbrs, idc, long_mrdb_df, mrdb_solution_field, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "print(\"--- Recommendations ---\")\n",
    "for s in absolute_sols:\n",
    "    print('{}.'.format(count), s)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "print(\"--- Recommendations ---\")\n",
    "for s in clusters_sols:\n",
    "    print('{}.'.format(count), s)\n",
    "    count +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Solution Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "K = range(1, 400, 10)\n",
    "for k in K:\n",
    "    sol_kmeans = KMeans(n_clusters=k, random_state=0).fit(sol_lda_data)\n",
    "    # For each point, determine distance to all centroids, determine the closest centroid (min), sum all \n",
    "    distortions.append(sum(np.min(cdist(sol_lda_data, sol_kmeans.cluster_centers_, 'euclidean'), axis=1)) / sol_lda_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "K = range(1, 400, 10)\n",
    "plt.plot(K, distortions, 'bo-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion by K clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 70 as best fit for number of clusters\n",
    "sol_topics = 70\n",
    "sol_kmeans = KMeans(n_clusters=sol_topics, random_state=0).fit(sol_lda_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "sol_distances = cdist(sol_lda_data, sol_kmeans.cluster_centers_, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_cluster_group = np.argmin(sol_distances, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_cluster_dict = defaultdict(list)\n",
    "for idx, v in enumerate(sol_cluster_group):\n",
    "    sol_cluster_dict[v].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(sol_lda_data)\n",
    "distances, sol_indices = sol_nbrs.kneighbors(sol_kmeans.cluster_centers_)\n",
    "cluster_labels ={}\n",
    "for i in range(len(sol_indices)):\n",
    "    cluster_labels[i] = str(long_mrdb_df[mrdb_solution_field].iloc[sol_indices[i]].values[0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cluster(cluster_labels, cluster_dict, cluster_idx, num, data, data_df, data_field):\n",
    "    \"\"\"\n",
    "        cluster_labels: Dict of cluster index to head label\n",
    "        cluster_dict: Dict of cluster index to elements placed into cluster\n",
    "        cluster_idx: cluster number\n",
    "        num: number to sample\n",
    "        data: full topic dataset\n",
    "        data_df: Dataframe of the source of solutions\n",
    "        data_field: The field name for solutions in dataframe\n",
    "        \n",
    "    \"\"\"\n",
    "    head = cluster_labels[cluster_idx]\n",
    "    print(\"HEAD:\", head)\n",
    "    sampled_indices = np.random.choice(cluster_dict[cluster_idx], num, replace=False)\n",
    "    for idx in sampled_indices:\n",
    "        print()\n",
    "        print(\"Trouble Ticket Number: \", data_df['JCN'].iloc[idx])\n",
    "        print(\"Problem Description: \", data_df[data_field].iloc[idx])\n",
    "\n",
    "view_cluster(cluster_labels, sol_cluster_dict, 1, 10, sol_lda_data, long_mrdb_df, mrdb_solution_field)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_cluster(cluster_labels, sol_cluster_dict, 44, 10, sol_lda_data, long_mrdb_df, mrdb_solution_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in sol_cluster_dict.items():\n",
    "    if 5566 in v:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_mrdb_df[long_mrdb_df[mrdb_solution_field].str.contains('PIGTAIL').fillna(False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster TAAS Problem Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions_prob = []\n",
    "K = range(1, 400, 10)\n",
    "for k in K:\n",
    "    prob_kmeans = KMeans(n_clusters=k, random_state=0).fit(taas_transformed)\n",
    "    # For each point, determine distance to all centroids, determine the closest centroid (min), sum all \n",
    "    distortions_prob.append(sum(np.min(cdist(taas_transformed, prob_kmeans.cluster_centers_, 'euclidean'), axis=1)) / taas_transformed.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow\n",
    "K = range(1, 400, 10)\n",
    "plt.plot(K, distortions_prob, 'bo-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Distortion by K clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using elbow at 80 for the best number of clusters\n",
    "prob_topics = 80\n",
    "prob_kmeans = KMeans(n_clusters=prob_topics, random_state=0).fit(taas_transformed)\n",
    "\n",
    "prob_distances = cdist(taas_transformed, prob_kmeans.cluster_centers_, 'euclidean')\n",
    "prob_cluster_group = np.argmin(prob_distances, axis=1)\n",
    "\n",
    "prob_cluster_dict = defaultdict(list)\n",
    "for idx, v in enumerate(prob_cluster_group):\n",
    "    prob_cluster_dict[v].append(idx)\n",
    "    \n",
    "prob_nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(taas_transformed)\n",
    "distances, prob_indices = prob_nbrs.kneighbors(prob_kmeans.cluster_centers_)\n",
    "prob_cluster_labels ={}\n",
    "for i in range(len(prob_indices)):\n",
    "    prob_cluster_labels[i] = str(long_taas_df[taas_field].iloc[prob_indices[i]].values[0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_cluster(prob_cluster_labels, prob_cluster_dict, 50, 10, taas_transformed, long_taas_df, taas_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Issues Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "issue_counts = sorted([(len(v), k) for k, v in prob_cluster_dict.items()], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pair of count of elements in cluster and cluster index\n",
    "issue_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the top n problem clusters\n",
    "\n",
    "for count, category in issue_counts[:n]:\n",
    "    print(\"Count:\", count, \"Cluster:\", category) \n",
    "    print(prob_cluster_labels[category], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Time to Implement Solution Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time_field = \"Mean Time Fieldname\"\n",
    "\n",
    "sol_implementation_time = {}\n",
    "for k, v in sol_cluster_dict.items():\n",
    "    sol_implementation_time[k] = long_mrdb_df[mean_time_field].iloc[v].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t, c in sorted([(v, k) for k, v in sol_implementation_time.items()]):\n",
    "    print(\"Mean Time to Resolve:\", t, \"Cluster:\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to cluster 66 were the fastest\n",
    "\n",
    "view_cluster(cluster_labels, sol_cluster_dict, 62, 10, sol_lda_data, long_mrdb_df, mrdb_solution_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Solutions for Problem Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_cluster_nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(sol_kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION USES VARIABLES DEFINED OUTSIDE OF THE FUNCTION\n",
    "\n",
    "def get_solution_recommendation_by_problem_cluster(cluster_idx, p_cluster_dict, num, nearestn, top_n=7):\n",
    "    \"\"\"\n",
    "        cluster_idx: Problem cluster index\n",
    "        p_cluster_dict: Dict of Problem cluster index to elements in cluster\n",
    "        num: Int number of elements to sample from problem clusters\n",
    "        nearestn: Int number of neighbors in MRDB to use, max is 10 based on lda_nbrs variable defined outside of this function\n",
    "    \"\"\"\n",
    "    sol_counts = defaultdict(int)\n",
    "    sampled_indices = np.random.choice(p_cluster_dict[cluster_idx], num, replace=True)\n",
    "    for idx in sampled_indices:\n",
    "        mrdb_indices = get_neighbors(lda_nbrs, idx, taas_transformed, long_taas_df, taas_field, long_mrdb_df, mrdb_field)\n",
    "        mrdb_indices = mrdb_indices[:nearestn]\n",
    "        for i in range(len(mrdb_indices)):\n",
    "            topic_sol = sol_lda.transform(solution_data[mrdb_indices[i]])\n",
    "            dist, cluster_index = sol_cluster_nbrs.kneighbors(topic_sol)\n",
    "            sol_counts[cluster_index[0][0]] += 1\n",
    "    \n",
    "    labels = []\n",
    "    sizes = []\n",
    "    count = 0\n",
    "    other_total = 0\n",
    "    sorted_solutions = sorted([(v, k) for k, v in sol_counts.items()], reverse=True)\n",
    "    for v, k in sorted_solutions:\n",
    "        if count < top_n:\n",
    "            labels.append(cluster_labels[k]+ \", {}\".format(np.round(sol_implementation_time[k], 2)))\n",
    "            sizes.append(v)\n",
    "            count += 1\n",
    "        else:\n",
    "            other_total+=v\n",
    "    if count:\n",
    "        labels.append(\"Other\")\n",
    "        sizes.append(other_total)\n",
    "        \n",
    "    print(labels)\n",
    "    \n",
    "    n_colors = len(labels)\n",
    "    rate = 1/n_colors\n",
    "    #ygb_cmap = plt.get_cmap(\"YlGnBu\")\n",
    "    #inner_colors = ygb_cmap(np.arange(n_colors)*15)\n",
    "    colors = []\n",
    "    red = (218/255, 4/255, 36/255, 255/255)\n",
    "    for i in range(n_colors):\n",
    "        new_red = list(red)\n",
    "        new_red[3] -= rate*i\n",
    "\n",
    "        colors.append(new_red)\n",
    "\n",
    "        \n",
    "    fig1, ax1 = plt.subplots(figsize=(20, 20))\n",
    "    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors,\n",
    "            shadow=True, startangle=90, textprops={'fontsize': 56})\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution clusters for problem cluster 50 (Antennas)\n",
    "\n",
    "get_solution_recommendation_by_problem_cluster(50, prob_cluster_dict, 100, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
